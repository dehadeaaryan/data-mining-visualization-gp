{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import multiprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('f1_cleaned.csv')\n",
    "data = data.rename(columns={'driver_name' : 'team_name'})\n",
    "data.drop(['code','driver_age','quali_mean'], axis=1, inplace=True)\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "#we are not using a random split here, training with pre 2024 data and trying to predict the races that occured in 2024\n",
    "\n",
    "train = data[data.year<2024].copy()\n",
    "test = data[data.year==2024].copy()\n",
    "\n",
    "#testing set\n",
    "y_test = test.pop('finishing_pos')\n",
    "x_test = test\n",
    "\n",
    "#training set\n",
    "y_train = train.pop('finishing_pos')\n",
    "x_train = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding vars and scaling data\n",
    "\n",
    "cat_feat = ['team_name',\n",
    "            'driver_nat', \n",
    "            'circuitRef']\n",
    "x_num_feat = ['year',\n",
    "              'starting_pos', \n",
    "              'laps', \n",
    "              'driver_dnf', \n",
    "              'car_dnf']\n",
    "\n",
    "#scale y later if needed for a distance model\n",
    "\n",
    "ct = ColumnTransformer(transformers=[\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', drop='first'), cat_feat), #avoid dummy var trap with OHE\n",
    "    ('scx', StandardScaler(), x_num_feat)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lasso Model (l1)\n",
    "\n",
    "#l1\n",
    "lasso = Pipeline(steps=[\n",
    "    ('preprocessing', ct),\n",
    "    ('model', Lasso())\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    \"model__fit_intercept\": [True, False],\n",
    "    \"model__positive\": [True, False],\n",
    "    \"model__alpha\": [.01,.1,1,10,100]\n",
    "    #\"model__alpha\": np.arange(.1,100.1,.1)\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "    'r2': 'r2',\n",
    "    'rmse': 'neg_root_mean_squared_error'\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(lasso, parameters, scoring=scoring, refit='r2', cv=10, return_train_score=True)\n",
    "\n",
    "gs.fit(x_train,y_train)\n",
    "\n",
    "best_gs_model = gs.best_estimator_\n",
    "\n",
    "\n",
    "y_pred = best_gs_model.predict(x_test)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print('R2;', r2)\n",
    "print('Root Mean Squared Error:', rmse)\n",
    "print('MAE;', mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coefficents based on L1\n",
    "\n",
    "lasso_model = best_gs_model.named_steps['model']\n",
    "\n",
    "print('lasso coeff', lasso_model.coef_)\n",
    "\n",
    "preprocessor = best_gs_model.named_steps['preprocessing']\n",
    "ridge_model = best_gs_model.named_steps['model']\n",
    "\n",
    "\n",
    "ohe = preprocessor.named_transformers_['encoder']\n",
    "ohe_feature_names = ohe.get_feature_names_out(cat_feat)\n",
    "\n",
    "\n",
    "feature_names = list(ohe_feature_names) + x_num_feat\n",
    "\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': ridge_model.coef_\n",
    "})\n",
    "\n",
    "\n",
    "coef_df['abs_coef'] = coef_df['Coefficient'].abs()\n",
    "coef_df = coef_df.sort_values(by='abs_coef', ascending=False).drop(columns='abs_coef')\n",
    "\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic LR model\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', ct),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print('R2;', r2)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', rmse)\n",
    "\n",
    "\n",
    "# Add predictions and actuals back to `test`\n",
    "test['pred_lr'] = y_pred\n",
    "test['finishing_pos'] = y_test  # reattach actuals for ranking\n",
    "\n",
    "# Convert actual and predicted to ranks within each race\n",
    "test['actual_rank'] = test.groupby('circuitRef')['finishing_pos'].rank(method='min')\n",
    "test['lr_rank'] = test.groupby('circuitRef')['pred_lr'].rank(method='min')\n",
    "\n",
    "# Calculate MAE and R² on ranks\n",
    "mae = mean_absolute_error(test['actual_rank'], test['lr_rank'])\n",
    "r2 = r2_score(test['actual_rank'], test['lr_rank'])\n",
    "\n",
    "print(\"Ranked MAE:\", mae)\n",
    "print(\"Ranked R²:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ridge Model (l2)\n",
    "\n",
    "ridge = Pipeline(steps=[\n",
    "    ('preprocessing', ct),\n",
    "    ('model', Ridge())\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    \"model__fit_intercept\": [True, False],\n",
    "    \"model__positive\": [True, False],\n",
    "    \"model__alpha\": [.01,.1,1,10,100]\n",
    "    #\"model__alpha\": np.arange(.1,100.1,.1)\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "    'r2': 'r2',\n",
    "    'rmse': 'neg_root_mean_squared_error'\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(ridge, parameters, scoring=scoring, refit='r2', cv=10, return_train_score=True)\n",
    "\n",
    "gs.fit(x_train,y_train)\n",
    "\n",
    "best_gs_model = gs.best_estimator_\n",
    "\n",
    "y_pred = best_gs_model.predict(x_test)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print('R2;', r2)\n",
    "print('Root Mean Squared Error:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df created to compare results \n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Team': test['team_name'],              \n",
    "    'Circut': test['circuitRef'],          \n",
    "    'Actual Pos': y_test,                         \n",
    "    'Predicted Pos': y_pred                       \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.copy()\n",
    "test['y_pred'] = y_pred\n",
    "test['y_true'] = y_test.values\n",
    "\n",
    "\n",
    "test['actual_rank'] = test.groupby('circuitRef')['y_true'].rank(method='min')\n",
    "test['predicted_rank'] = test.groupby('circuitRef')['y_pred'].rank(method='min')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x='actual_rank', y='predicted_rank', hue='team_name', data=test)\n",
    "\n",
    "#regression line\n",
    "plt.plot([1, test['actual_rank'].max()], [1, test['actual_rank'].max()], 'r--', label='Perfect Prediction')\n",
    "\n",
    "plt.xlabel('Actual Race Rank')\n",
    "plt.ylabel('Predicted Race Rank')\n",
    "plt.title('Predicted vs Actual Rankings by Team')\n",
    "plt.legend(title='Team', prop={'size': 8}, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding team colors\n",
    "team_colors = {\n",
    "    'Red Bull': '#1E41FF',\n",
    "    'Mercedes': '#00D2BE',\n",
    "    'Ferrari': '#DC0000',\n",
    "    'McLaren': '#FF8700',\n",
    "    'Aston Martin': '#006F62',\n",
    "    'Alpine F1 Team': '#0090FF',\n",
    "    'Williams': '#005AFF',\n",
    "    'RB F1 Team': '#6692FF',\n",
    "    'Haas F1 Team': '#B6BABD',\n",
    "    'Sauber': '#52E252',\n",
    "    #fallback color\n",
    "    'Other': '#888888'\n",
    "}\n",
    "palette = {team: color for team, color in team_colors.items() if team in test['team_name'].unique()}\n",
    "\n",
    "#getting unique races\n",
    "races = test['circuitRef'].unique()\n",
    "n_races = len(races)\n",
    "\n",
    "cols = 3\n",
    "rows = math.ceil(n_races / cols)\n",
    "\n",
    "plt.figure(figsize=(6 * cols, 5 * rows))\n",
    "\n",
    "for idx, race in enumerate(races):\n",
    "    ax = plt.subplot(rows, cols, idx + 1)\n",
    "    \n",
    "    race_data = test[test['circuitRef'] == race]\n",
    "    \n",
    "    sns.scatterplot(\n",
    "        x='actual_rank', \n",
    "        y='predicted_rank', \n",
    "        hue='team_name', \n",
    "        data=race_data, \n",
    "        ax=ax,\n",
    "        palette=palette,\n",
    "        legend=False\n",
    "    )\n",
    "    \n",
    "    #regression line\n",
    "    ax.plot([1, race_data['actual_rank'].max()], [1, race_data['actual_rank'].max()], 'r--')\n",
    "    \n",
    "    #scale\n",
    "    max_rank = int(max(race_data['actual_rank'].max(), race_data['predicted_rank'].max())) + 1\n",
    "    ax.set_xticks(range(1, max_rank + 1))\n",
    "    ax.set_yticks(range(1, max_rank + 1))\n",
    "    \n",
    "    ax.set_xlim(0.8, max_rank + 0.2)\n",
    "    ax.set_ylim(0.8, max_rank + 0.2)\n",
    "\n",
    "    ax.set_title(f'{race.capitalize()}')\n",
    "    ax.set_xlabel('Actual Rank')\n",
    "    ax.set_ylabel('Predicted Rank')\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Race-by-Race: Actual vs Predicted Driver Rankings by Team', fontsize=16, y=1.02)\n",
    "plt.legend(title='Team', bbox_to_anchor=(1.05, 1), loc='upper left', prop={'size': 8})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_to_points(pos):\n",
    "    points_map = {1: 25, 2: 18, 3: 15, 4: 12, 5: 10,\n",
    "                  6: 8, 7: 6, 8: 4, 9: 2, 10: 1}\n",
    "    return points_map.get(pos, 0)\n",
    "\n",
    "test['actual_points'] = test['actual_rank'].apply(position_to_points)\n",
    "test['predicted_points'] = test['predicted_rank'].apply(position_to_points)\n",
    "\n",
    "team_points = test.groupby(['team_name', 'circuitRef']).agg({\n",
    "    'actual_points': 'sum',\n",
    "    'predicted_points': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "\n",
    "season_totals = team_points.groupby('team_name').agg({\n",
    "    'actual_points': 'sum',\n",
    "    'predicted_points': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "#sorting by actual\n",
    "season_totals = season_totals.sort_values(by='actual_points', ascending=False)\n",
    "\n",
    "\n",
    "teams = season_totals['team_name']\n",
    "x = np.arange(len(teams))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.bar(x - width/2, season_totals['actual_points'], width, label='Actual')\n",
    "ax.bar(x + width/2, season_totals['predicted_points'], width, label='Predicted')\n",
    "\n",
    "ax.set_ylabel('Total Points')\n",
    "ax.set_title('Constructor Championship: Actual vs Predicted Points')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(teams, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(season_totals)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
